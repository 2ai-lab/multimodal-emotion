{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c687e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cb0c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "import librosa\n",
    "from torchaudio.utils import download_asset\n",
    "import cv2\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import lightning.pytorch as pl\n",
    "from torchaudio.transforms import *\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n",
    "from transformers import AutoFeatureExtractor, ASTForAudioClassification\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from emotion_utils.utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d3f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llogger = CSVLogger(\"emotions\", 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "545bd96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'IEMOCAP_full_release/Session*/sentences/wav/Ses*/*.wav'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09b6cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path_reg = 'IEMOCAP_full_release/Session*/dialog/EmoEvaluation/Categorical/Ses*.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9553880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues_path_reg = 'IEMOCAP_full_release/Session*/dialog/transcriptions/Ses*.txt'\n",
    "dialogues_paths = glob.glob(dialogues_path_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24efe34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues_path_reg = 'IEMOCAP_full_release/Session*/dialog/transcriptions/Ses*.txt'\n",
    "dialogues_paths = glob.glob(dialogues_path_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e57926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = get_meta(csv_file_path_reg, 'category')\n",
    "dial_df  =get_meta(dialogues_path_reg, 'dial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39d7ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.merge(cat_df, dial_df, on = 'file', how = 'inner').groupby(['file','category','dial']).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "090689df",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['category'] = meta_df['category'].str.replace(';.*','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df91a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta_df.groupby(['file','dial']).head(1).reset_index().drop(columns = ['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94bbea14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10039"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(meta_df['file'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe42f99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>category</th>\n",
       "      <th>dial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6613</th>\n",
       "      <td>Ses04F_script02_2_F000</td>\n",
       "      <td>anger</td>\n",
       "      <td>About what?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        file category          dial\n",
       "6613  Ses04F_script02_2_F000    anger   About what?"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df[meta_df['file']=='Ses04F_script02_2_F000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab7c5a9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asset = AudioDataset(meta_df,data_path, modality = 'audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "506f70ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_values': tensor([[[-0.7030, -1.0522, -0.6753,  ..., -1.1763, -1.2540, -1.1932],\n",
       "          [-0.8457, -1.2658, -0.8890,  ..., -1.2107, -1.2378, -1.1702],\n",
       "          [-1.0629, -1.2776, -1.1097,  ..., -1.1437, -1.2557, -1.2330],\n",
       "          ...,\n",
       "          [ 0.4670,  0.4670,  0.4670,  ...,  0.4670,  0.4670,  0.4670],\n",
       "          [ 0.4670,  0.4670,  0.4670,  ...,  0.4670,  0.4670,  0.4670],\n",
       "          [ 0.4670,  0.4670,  0.4670,  ...,  0.4670,  0.4670,  0.4670]]])},\n",
       " 0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0275055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LighteningModel(pl.LightningModule):\n",
    "    def __init__(self, input_size=48000, num_classes=10, hidden_size = 200, num_heads = 5, num_layers_tx  = 2):\n",
    "        super(LighteningModel, self).__init__()\n",
    "        self.train_r2 = torchmetrics.R2Score()\n",
    "        self.val_r2 = torchmetrics.R2Score()\n",
    "        self.train_auc = torchmetrics.classification.AUROC(task = 'multiclass', num_classes=num_classes)\n",
    "        self.val_auc = torchmetrics.classification.AUROC(task = 'multiclass', num_classes=num_classes)\n",
    "        \n",
    "        feature_extractor = AutoFeatureExtractor.from_pretrained(\"ast-finetuned-audioset-10-10-0.4593\")\n",
    "        self.sp_model = ASTForAudioClassification.from_pretrained(\"ast-finetuned-audioset-10-10-0.4593\", return_dict=False)\n",
    "        for param in self.sp_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.sp_model.classifier.dense = nn.Linear(768,527)\n",
    "        \n",
    "        self.fc1 = nn.Linear(527, num_classes)\n",
    "        self.dropout = nn.Dropout(.5) \n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self,x,):\n",
    "        x = x['input_values'].view(x['input_values'].size(0), 1024,128)\n",
    "        \n",
    "        out = self.sp_model(x, return_dict=False)[0] #256\n",
    "        out = self.dropout(self.fc1(out))\n",
    "#         out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.0001, weight_decay=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        signal, labels = train_batch\n",
    "#         signal = signal['input_values'].view(signal['input_values'].size(0), 1024,128).shape\n",
    "        labels =labels.float().to('cuda:0')\n",
    "        outputs = self(signal).to('cuda:0')#.argmax(1).float()\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        self.train_auc(outputs, labels.int())\n",
    "        self.log('loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log('train_auc', self.train_auc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        signal ,labels = val_batch\n",
    "        labels = labels.float().to('cuda:0')\n",
    "#         signal = signal['input_values'].view(signal['input_values'].size(0), 1024,128).shape\n",
    "        \n",
    "        outputs = self(signal).to('cuda:0')#.argmax(1).float()\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        self.val_auc(outputs, labels.int())\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log('val_auc', self.val_auc, on_step=False, on_epoch=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbc6caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LighteningModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec946d",
   "metadata": {},
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d9f4a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f0073a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(meta_df,data_path, modality = 'audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "954e7da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(dataset, [0.8, 0.2],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d54c5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd9d4b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a32ad13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler = WeightedRandomSampler(sample_weights, int(len(train_set)*1.5), replacement=True)\n",
    "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle = True, num_workers=8, drop_last=True, )\n",
    "test_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer( max_epochs=500, gradient_clip_val=0, logger = llogger, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79091b25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloader, test_dataloader, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
