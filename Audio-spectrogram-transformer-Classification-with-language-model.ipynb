{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c687e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cb0c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.utils import download_asset\n",
    "import cv2\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import lightning.pytorch as pl\n",
    "from torchaudio.transforms import *\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n",
    "from transformers import AutoFeatureExtractor, ASTForAudioClassification\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d3f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llogger = CSVLogger(\"emotions\", 'classification-lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "545bd96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'IEMOCAP_full_release/Session*/sentences/wav/Ses*/*.wav'\n",
    "files = glob.glob(data_path)\n",
    "file_name = [x.split(os.sep)[-1].split('.')[0] for x in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09b6cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_paths = 'IEMOCAP_full_release/Session*/dialog/EmoEvaluation/Categorical/Ses*.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5706aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues_path_reg = 'IEMOCAP_full_release/Session*/dialog/transcriptions/Ses*.txt'\n",
    "dialogues_paths = glob.glob(dialogues_path_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "921f3072",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = glob.glob(csv_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f3059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3844327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_category_from_path(path):\n",
    "    file_name_list = []\n",
    "    emotion_list = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            file_name_list.append(re.findall('Ses.*[A-Z][0-9]{3}', line)[0])\n",
    "            emotion_list.append(re.findall(':.*;', line)[0][1:-1].lower())\n",
    "    return file_name_list, emotion_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52516a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_dial_from_path(path):\n",
    "    file_name_list = []\n",
    "    dial_list = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            file_name_list.append(re.findall('Ses.*[A-Z][0-9]{3}', line)[0])\n",
    "            dial_list.append(re.findall(':.*', line)[0][1:])\n",
    "    return file_name_list, dial_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53af3600",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = []\n",
    "category_list = []\n",
    "for file in csv_files:\n",
    "    f, e = get_name_category_from_path(file)\n",
    "    file_list.extend(f)\n",
    "    category_list.extend(e)\n",
    "cat_df = pd.DataFrame(\n",
    "    {\n",
    "        'file':file_list,\n",
    "        'category':category_list\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f326d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = []\n",
    "dial_list = []\n",
    "for file in dialogues_paths:\n",
    "    try:\n",
    "        f, d = get_name_dial_from_path(file)\n",
    "        file_list.extend(f)\n",
    "        dial_list.extend(d)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "dial_df = pd.DataFrame(\n",
    "    {\n",
    "        'file':file_list,\n",
    "        'dial':dial_list\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "090689df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df['category'] = cat_df['category'].str.replace(';.*','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "774d5886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30117,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_df['category'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1977ceb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Ses04F_script02_2_F000</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3892</th>\n",
       "      <td>Ses04F_script02_2_F000</td>\n",
       "      <td>frustration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4985</th>\n",
       "      <td>Ses04F_script02_2_F000</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        file     category\n",
       "134   Ses04F_script02_2_F000        anger\n",
       "3892  Ses04F_script02_2_F000  frustration\n",
       "4985  Ses04F_script02_2_F000        other"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_df[cat_df['file']=='Ses04F_script02_2_F000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b04b59da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_df = pd.merge(cat_df, dial_df, on = 'file').groupby(['file','category','dial']).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75d7b163",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral state', 'surprise', 'anger', 'frustration', 'disgust',\n",
       "       'other', 'sadness', 'happiness', 'fear', 'excited'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99e60dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Custom dataset class for loading audio dataset.\n",
    "        meta_df: dataframe containing file_name (without \n",
    "                the .wav extension) and the category (labels) \n",
    "        directory: regular expression for the directory to look \n",
    "                for wav files. (e.g. /dataset/speech/*.wav)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, meta_df, directory, **kwargs):\n",
    "        self.meta_df = meta_df\n",
    "        self.directory = directory\n",
    "#         self.transforms = transforms\n",
    "        self.audio_path_list = glob.glob(directory)\n",
    "#         self.signal_trim = signal_trim\n",
    "        category = self.meta_df['category'].unique()\n",
    "        self.t_dict = dict(zip(category,range(len(category))))\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.meta_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         t_dict = dict(zip(\n",
    "#                 self.category,\n",
    "#                     range(len(self.category))\n",
    "#                 ))\n",
    "        audio_path = self.audio_path_list[idx]\n",
    "        audio_name = os.path.basename(audio_path).split('.')[0]\n",
    "        dialogue = self.meta_df.loc[idx,['dial']].values[0]\n",
    "        targets = self.meta_df.loc[idx,[ 'category']].values[0]\n",
    "        \n",
    "        signal, sr = torchaudio.load(audio_path, )\n",
    "        signal =  signal[0]\n",
    "        \n",
    "        feature_extractor = AutoFeatureExtractor.from_pretrained(\"ast-finetuned-audioset-10-10-0.4593\")\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        audio_feature = feature_extractor(signal, sampling_rate=sr, return_tensors=\"pt\")\n",
    "        encoded_text = tokenizer(dialogue,return_tensors=\"pt\",  padding=\"max_length\", max_length=50, add_special_tokens=True, truncation=True,)\n",
    "#         print(encoded_text)\n",
    "        return audio_feature, encoded_text, self.t_dict[targets]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33aa688b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "129536b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab7c5a9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asset = AudioDataset(meta_df,data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b902d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(dataloader))[1]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0275055",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LighteningModel(pl.LightningModule):\n",
    "    def __init__(self, input_size=48000, num_classes=10, hidden_size = 200, num_heads = 5, num_layers_tx  = 2):\n",
    "        super(LighteningModel, self).__init__()\n",
    "        self.train_r2 = torchmetrics.R2Score()\n",
    "        self.val_r2 = torchmetrics.R2Score()\n",
    "        self.train_auc = torchmetrics.classification.AUROC(task = 'multiclass', num_classes=num_classes)\n",
    "        self.val_auc = torchmetrics.classification.AUROC(task = 'multiclass', num_classes=num_classes)\n",
    "        \n",
    "        self.val_f1 = torchmetrics.classification.F1Score(task = 'multiclass', num_classes=num_classes)\n",
    "        self.train_f1 = torchmetrics.classification.F1Score(task = 'multiclass', num_classes=num_classes)\n",
    "        \n",
    "        \n",
    "        feature_extractor = AutoFeatureExtractor.from_pretrained(\"ast-finetuned-audioset-10-10-0.4593\")\n",
    "        self.sp_model = ASTForAudioClassification.from_pretrained(\"ast-finetuned-audioset-10-10-0.4593\", return_dict=False)\n",
    "        \n",
    "        self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        for param in self.sp_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.sp_model.classifier.dense = nn.Linear(768,256)\n",
    "        \n",
    "        for param in self.bert_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        for param in self.bert_model.encoder.layer[-1:].parameters():\n",
    "            param.requires_grad = True\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "        self.fc1 = nn.Linear(1024, num_classes)\n",
    "        self.dropout = nn.Dropout(.7) \n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self,x,y):\n",
    "        x = x['input_values'].view(x['input_values'].size(0), 1024,128)\n",
    "        input_bert = y['input_ids'].view(y['input_ids'].size(0), 50)\n",
    "        atten_bert =  y['attention_mask'].view(y['attention_mask'].size(0), 50)\n",
    "#         y = y['input_ids']\n",
    "#         print(y.shape)\n",
    "        sp = self.sp_model(x, return_dict=False)[0] #256\n",
    "        bert = self.bert_model(input_ids = input_bert, attention_mask = atten_bert, return_dict = False)[0]\n",
    "        bert = self.dropout(bert.mean(1))\n",
    "        sp = self.dropout(sp)\n",
    "\n",
    "        out = torch.concat([bert, sp], axis = 1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.0001, weight_decay=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        signal, dial, labels = train_batch\n",
    "#         signal = signal['input_values'].view(signal['input_values'].size(0), 1024,128).shape\n",
    "        labels =labels.float().to('cuda:0')\n",
    "        outputs = self(signal, dial).to('cuda:0')#.argmax(1).float()\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        self.train_auc(outputs, labels.int())\n",
    "        self.train_f1(outputs, labels.int())\n",
    "        \n",
    "        self.log('loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log('train_auc', self.train_auc, on_step=False, on_epoch=True)\n",
    "        self.log('train_f1', self.train_f1, on_step=False, on_epoch=True)\n",
    "        \n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        signal ,dial, labels = val_batch\n",
    "        labels = labels.float().to('cuda:0')\n",
    "#         signal = signal['input_values'].view(signal['input_values'].size(0), 1024,128).shape\n",
    "        \n",
    "        outputs = self(signal, dial).to('cuda:0')#.argmax(1).float()\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        loss = criterion(outputs, labels.long())\n",
    "        self.val_auc(outputs, labels.int())\n",
    "        self.val_f1(outputs, labels.int())\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log('val_auc', self.val_auc, on_step=False, on_epoch=True)\n",
    "        self.log('val_f1', self.val_f1, on_step=False, on_epoch=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbc6caef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = LighteningModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76520a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d9f4a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f0073a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(meta_df,data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cefef7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9989"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f92675f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9989, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "954e7da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(dataset, [0.8, 0.2],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a32ad13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler = WeightedRandomSampler(sample_weights, int(len(train_set)*1.5), replacement=True)\n",
    "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle = True, num_workers=8, drop_last=True, )\n",
    "test_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f3d7482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer( max_epochs=500, gradient_clip_val=0, logger = llogger, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79091b25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name       | Type                      | Params\n",
      "----------------------------------------------------------\n",
      "0  | train_r2   | R2Score                   | 0     \n",
      "1  | val_r2     | R2Score                   | 0     \n",
      "2  | train_auc  | MulticlassAUROC           | 0     \n",
      "3  | val_auc    | MulticlassAUROC           | 0     \n",
      "4  | val_f1     | MulticlassF1Score         | 0     \n",
      "5  | train_f1   | MulticlassF1Score         | 0     \n",
      "6  | sp_model   | ASTForAudioClassification | 86.4 M\n",
      "7  | bert_model | BertModel                 | 109 M \n",
      "8  | fc1        | Linear                    | 10.2 K\n",
      "9  | dropout    | Dropout                   | 0     \n",
      "10 | relu       | ReLU                      | 0     \n",
      "----------------------------------------------------------\n",
      "7.3 M     Trainable params\n",
      "188 M     Non-trainable params\n",
      "195 M     Total params\n",
      "783.513   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ef1a2a448c40228470c790520766b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary +: 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;241;43m+\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary +: 'NoneType'"
     ]
    }
   ],
   "source": [
    "+trainer.fit(model, train_dataloader, test_dataloader, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2c63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabdfc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
